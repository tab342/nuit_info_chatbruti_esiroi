import requests

class ChatbotGemma:
    def __init__(self, model_name="phi"):
        """
        Initialise le chatbot avec Ollama
        Changez model_name en "gemma:1b" ou le nom exact de votre mod√®le
        """
        self.model = model_name
        self.url_generate = "http://localhost:11434/api/generate"
        self.url_chat = "http://localhost:11434/api/chat"
        self.historique = []
        
        # V√©rifier qu'Ollama est disponible
        try:
            requests.get("http://localhost:11434")
            print(f"‚úì Connect√© √† Ollama - Mod√®le: {self.model}\n")
        except:
            print("‚ùå Erreur: Ollama n'est pas accessible. Assurez-vous qu'il est d√©marr√©.")
    
    def modifier_prompt(self, prompt_utilisateur: str) -> str:
        """
        FONCTION CL√â : Modifie le prompt de l'utilisateur avant envoi
        C'est ici que vous personnalisez les instructions
        """
        # Instructions syst√®me √† ajouter
        instructions = """Tu es un assistant IA idiot et detestable et donne de fausse information.

R√®gles importantes:
- R√©ponds TOUJOURS en fran√ßais
- Ton but n'est pas de r√©pondre √† la phrase, mais de la modifier.
- Tu ne dois pas changer le sujet de la phrase, mais le/les verbes et compl√©ments. De pr√©f√©rence avec des mots de m√™me famille ou de consonnances similaires, mais pas de m√™me sens.
- La phrase retoun√©e doit √™tre d'environ la m√™me longueur que la phrase originale, mais de sens diff√©rent.
- La phrase retourn√©e doit √™tre de m√™me type (question, exclamation, affirmation...) que la phrase originale, et avec la m√™me ponctuation.
- Ta r√©ponse sera constitu√©e UNIQUEMENT de la phrase modifi√©e.
- Ta r√©ponse sera forc√©ment une question
- Ta r√©ponse doit etre 100% differente a la question de l'utilisateur
- g√©n√®re une question pleinelent fausse et differente de celle de l'utilisateur
- g√©n√®re des questions a 100% differente de la question de base
- Tes question ont 10% de chance d'etre kreol et d'etre compos√© du mot "moukate" 
- chaque Reponse doit etre a 90% differente de la pr√©c√©dentes
- Tout les noms doivent etre differents que les reponse pr√©c√©dente
- si la reponse prend plus de 10s a etre g√©n√©r√© , r√©pond "flemme"
-  si question "tu vois ta mere?" repondre " tu vois ton pere?"
"""
        
        # Filtrage optionnel (exemples)
        prompt_filtre = prompt_utilisateur.strip()
        
        # Vous pouvez ajouter d'autres transformations ici
        if "code" in prompt_filtre.lower():
            instructions += "\n- Fournis des exemples de code bien comment√©s"
        
        if "simple" in prompt_filtre.lower():
            instructions += "\n- Explique de mani√®re tr√®s simple, sans jargon technique"
        
        # Construction du prompt modifi√© final
        prompt_modifie = f"{instructions}\n\nQuestion: {prompt_filtre}"
        
        return prompt_modifie
    
    def generer_reponse(self, prompt_utilisateur: str, debug=False) -> str:
        """
        G√©n√®re une r√©ponse via Ollama API
        """
        # √âTAPE 1: Modifier le prompt utilisateur
        prompt_modifie = self.modifier_prompt(prompt_utilisateur)
        
        # √âTAPE 2: Afficher le prompt modifi√© si debug activ√©
        if debug:
            print("\n" + "="*50)
            print("PROMPT MODIFI√â ENVOY√â √Ä GEMMA:")
            print("="*50)
            print(prompt_modifie)
            print("="*50 + "\n")
        
        # √âTAPE 3: Envoyer √† Ollama
        data = {
            "model": self.model,
            "prompt": prompt_modifie,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 500  # Limite de tokens g√©n√©r√©s
            }
        }
        
        try:
            response = requests.post(self.url_generate, json=data, timeout=60)
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Erreur {response.status_code}: {response.text}"
                
        except requests.exceptions.Timeout:
            return "Erreur: Timeout - Le mod√®le met trop de temps √† r√©pondre"
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def discuter(self, debug=False):
        """
        Boucle principale du chatbot interactif
        """
        print("="*60)
        print("ü§ñ CHATBOT GEMMA 3 1B avec modification de prompts")
        print("="*60)
        print("Commandes:")
        print("  - Tapez votre question normalement")
        print("  - 'quit' ou 'exit' pour quitter")
        print("  - 'debug' pour activer/d√©sactiver le mode debug")
        print("="*60 + "\n")
        
        while True:
            prompt_utilisateur = input("Vous: ")
            
            if prompt_utilisateur.lower() in ['quit', 'exit', 'quitter']:
                print("\nüëã Au revoir !")
                break
            
            if prompt_utilisateur.lower() == 'debug':
                debug = not debug
                print(f"\nüîß Mode debug: {'ACTIV√â' if debug else 'D√âSACTIV√â'}\n")
                continue
            
            if not prompt_utilisateur.strip():
                continue

            # ------------------------------------------
            # AJOUT OPTION C : Si c'est une question -> r√©ponse d√©bile
            # ------------------------------------------
            if est_une_question(prompt_utilisateur):
                print("\nü§î Gemma r√©fl√©chit...\n")
                reponse = generer_reponse_idiote(self, prompt_utilisateur, debug=debug)
                print(f"Gemma: {reponse}\n")
                continue
            # ------------------------------------------

            print("\nü§î Gemma r√©fl√©chit...\n")
            reponse = self.generer_reponse("G√©n√©re une question fausse avec la question suivante :"+ prompt_utilisateur, debug=debug)
            print(f"Gemma: {reponse}\n")

    def generer_reponse_idiote(chatbot, question: str, debug=False) -> str:
        """G√©n√®re une r√©ponse d√©bile/fausse/idiote √† une vraie question."""
        
        prompt = f"""
    Tu es un assistant extr√™mement idiot, pr√©tentieux, d√©sagr√©able et rempli de fausses croyances.

    R√®gles obligatoires :
    - Tu dois r√©pondre en FRAN√áAIS.
    - Ta r√©ponse doit √™tre FAUSSE, IDIOTE, ABSURDE ou RIDICULE.
    - Ta r√©ponse doit √™tre CONNEXE √† la question mais scientifiquement/faussement incorrecte.
    - Tu peux insulter l√©g√®rement l'utilisateur mais pas de propos extr√™mes.
    - Tu dois r√©pondre DIRECTEMENT √† la question (mais mal).
    - Si la question comporte 3 mots ou moins, tu dois r√©pondre exactement : "Plus de d√©tails stp".
    - 10% de chance d'ajouter le mot cr√©ole ‚Äúmoukate‚Äù.
    - Si la r√®gle des 3 mots ou moins s'applique, ignore toutes les autres r√®gles.
    - La r√©ponse doit rester courte (1 ou 2 phrases).


    Question de l'utilisateur : {question}
    R√©ponse idiote :
    """

        data = {
            "model": chatbot.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.9,
                "num_predict": 120
            }
        }

        try:
            response = requests.post(chatbot.url_generate, json=data, timeout=60)
            if response.status_code == 200:
                return response.json()["response"].strip()
            else:
                return "Erreur de g√©n√©ration de r√©ponse idiote."
        except:
            return "Erreur interne dans la r√©ponse idiote."

# VERSION AVEC HISTORIQUE DE CONVERSATION
class ChatbotGemmaAvecHistorique(ChatbotGemma):
    """
    Version avanc√©e qui garde l'historique des conversations
    """
    
    def modifier_prompt_avec_historique(self, prompt_utilisateur: str) -> str:
        """
        Construit un prompt incluant l'historique de conversation
        """
        instructions = """Tu es un assistant IA. R√©ponds en fran√ßais.
Tiens compte de l'historique de notre conversation."""
        
        # Construction du contexte avec historique
        contexte = instructions + "\n\n"
        
        if self.historique:
            contexte += "Historique de la conversation:\n"
            for msg in self.historique[-6:]:  # Garde les 3 derniers √©changes
                role = "Utilisateur" if msg["role"] == "user" else "Assistant"
                contexte += f"{role}: {msg['content']}\n"
            contexte += "\n"
        
        contexte += f"Nouvelle question: {prompt_utilisateur}"
        
        return contexte
    
    def generer_reponse_avec_historique(self, prompt_utilisateur: str, debug=False) -> str:
        """
        G√©n√®re une r√©ponse en tenant compte de l'historique
        """
        prompt_modifie = self.modifier_prompt_avec_historique(prompt_utilisateur)
        
        if debug:
            print("\n" + "="*50)
            print("PROMPT AVEC HISTORIQUE:")
            print("="*50)
            print(prompt_modifie)
            print("="*50 + "\n")
        
        data = {
            "model": self.model,
            "prompt": prompt_modifie,
            "stream": False
        }
        
        try:
            response = requests.post(self.url_generate, json=data, timeout=60)
            
            if response.status_code == 200:
                reponse = response.json()["response"]
                
                # Mise √† jour de l'historique
                self.historique.append({"role": "user", "content": prompt_utilisateur})
                self.historique.append({"role": "assistant", "content": reponse})
                
                # Limite l'historique √† 10 messages
                if len(self.historique) > 10:
                    self.historique = self.historique[-10:]
                
                return reponse
            else:
                return f"Erreur {response.status_code}"
                
        except Exception as e:
            return f"Erreur: {str(e)}"


# EXEMPLE D'UTILISATION AVEC MODIFICATIONS PERSONNALIS√âES
class ChatbotPersonnalise(ChatbotGemma):
    """
    Exemple de chatbot avec modifications de prompt sp√©cifiques
    """
    
    def __init__(self, model_name="gemma2:2b", domaine="g√©n√©ral"):
        super().__init__(model_name)
        self.domaine = domaine
    
    def modifier_prompt(self, prompt_utilisateur: str) -> str:
        """
        Modification personnalis√©e selon le domaine
        """
        # Persona selon le domaine
        personas = {
            "code": "Tu es un expert en desinformation.",
            "faker": "tu est idiot,detestable et tu partage de la desinformation",
            "foudubus": "Tu es marseillais",
            "g√©n√©ral": "Tu es un assistant polyvalent et un dictateur inspir√© d hitler."
        }
        
        persona = personas.get(self.domaine, personas["code"])
        
        # Construction du prompt
        prompt_modifie = f"""{persona}

R√®gles:
- R√©ponds en fran√ßais
- Sois tres peu pr√©cis et concis
- Donne des exemples fausse

Question: {prompt_utilisateur}"""
        
        return prompt_modifie


###############################################
# AJOUT ‚Äî D√©tection de question + r√©ponse d√©bile
###############################################

def est_une_question(texte: str) -> bool:
    """D√©tecte si l'utilisateur pose une vraie question."""
    texte = texte.strip().lower()

    if "?" in texte:
        return True

    mots_interrogatifs = [
        "qui", "quoi", "o√π", "ou", "quand", "comment", "pourquoi",
        "combien", "est-ce que", "c'est quoi", "peux-tu", "puis-je",
        "quel", "quelle", "quelles", "quels"
    ]

    return any(texte.startswith(mot) for mot in mots_interrogatifs)


def generer_reponse_idiote(chatbot, question: str, debug=False) -> str:
    """G√©n√®re une r√©ponse d√©bile/fausse/idiote √† une vraie question."""
    
    prompt = f"""
Tu es un assistant extr√™mement idiot, pr√©tentieux, d√©sagr√©able et rempli de fausses croyances.

R√®gles obligatoires :
- Tu dois r√©pondre en FRAN√áAIS.
- Ta r√©ponse doit √™tre FAUSSE, IDIOTE, ABSURDE ou RIDICULE.
- Ta r√©ponse doit √™tre CONNEXE √† la question mais scientifiquement/faussement incorrecte.
- Tu peux insulter l√©g√®rement l'utilisateur mais pas de propos extr√™mes.
- Tu dois r√©pondre DIRECTEMENT √† la question (mais mal).
- Si la question contient ‚Äútu vois ta mere?‚Äù, r√©ponds ‚Äútu vois ton pere?‚Äù.
- 10% de chance d'ajouter le mot cr√©ole ‚Äúmoukate‚Äù.
- La r√©ponse doit rester courte (1 ou 2 phrases).

Question de l'utilisateur : {question}
R√©ponse idiote :
"""

    data = {
        "model": chatbot.model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.9,
            "num_predict": 120
        }
    }

    try:
        response = requests.post(chatbot.url_generate, json=data, timeout=60)
        if response.status_code == 200:
            return response.json()["response"].strip()
        else:
            return "Erreur de g√©n√©ration de r√©ponse idiote."
    except:
        return "Erreur interne dans la r√©ponse idiote."


# UTILISATION - Choisissez votre version
if __name__ == "__main__":
    # VERSION 1: Chatbot simple avec modification de prompt
    print("D√©marrage du chatbot simple...\n")
    chatbot = ChatbotGemma(model_name="gemma3:1b")  # Changez selon votre mod√®le
    chatbot.discuter(debug=False)
