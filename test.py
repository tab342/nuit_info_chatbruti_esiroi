import requests
import json

class ChatbotGemma:
    def __init__(self, model_name="phi"):
        """
        Initialise le chatbot avec Ollama
        Changez model_name en "gemma:1b" ou le nom exact de votre mod√®le
        """
        self.model = model_name
        self.url_generate = "http://localhost:11434/api/generate"
        self.url_chat = "http://localhost:11434/api/chat"
        self.historique = []
        
        # V√©rifier qu'Ollama est disponible
        try:
            requests.get("http://localhost:11434")
            print(f"‚úì Connect√© √† Ollama - Mod√®le: {self.model}\n")
        except:
            print("‚ùå Erreur: Ollama n'est pas accessible. Assurez-vous qu'il est d√©marr√©.")
    
    def modifier_prompt(self, prompt_utilisateur: str) -> str:
        """
        FONCTION CL√â : Modifie le prompt de l'utilisateur avant envoi
        C'est ici que vous personnalisez les instructions
        """
        # Instructions syst√®me √† ajouter
        instructions = """Tu es un assistant IA expert et serviable.

R√®gles importantes:
- R√©ponds TOUJOURS en fran√ßais
- Ton but n'est pas de r√©pondre √† la phrase, mais de la modifier.
- Tu ne dois pas changer le sujet de la phrase, mais le/les verbes et compl√©ments. De pr√©f√©rence avec des mots de m√™me famille ou de consonnances similaires, mais pas de m√™me sens.
- La phrase retoun√©e doit √™tre d'environ la m√™me longueur que la phrase originale, mais de sens diff√©rent.
- La phrase retourn√©e doit √™tre de m√™me type (question, exclamation, affirmation...) que la phrase originale, et avec la m√™me ponctuation.
- Ta r√©ponse sera constitu√©e UNIQUEMENT de la phrase modifi√©e.
- Ta r√©ponse sera forc√©ment une question
- Ta r√©ponse doit etre 100% differente a la question de l'utilisateur
- g√©n√®re une question pleinelent fausse et differente de celle de l'utilisateur
"""
        
        # Filtrage optionnel (exemples)
        prompt_filtre = prompt_utilisateur.strip()
        
        # Vous pouvez ajouter d'autres transformations ici
        # Par exemple: d√©tecter des mots-cl√©s et adapter le contexte
        if "code" in prompt_filtre.lower():
            instructions += "\n- Fournis des exemples de code bien comment√©s"
        
        if "simple" in prompt_filtre.lower():
            instructions += "\n- Explique de mani√®re tr√®s simple, sans jargon technique"
        
        # Construction du prompt modifi√© final
        prompt_modifie = f"{instructions}\n\nQuestion: {prompt_filtre}"
        
        return prompt_modifie
    
    def generer_reponse(self, prompt_utilisateur: str, debug=False) -> str:
        """
        G√©n√®re une r√©ponse via Ollama API
        """
        # √âTAPE 1: Modifier le prompt utilisateur
        prompt_modifie = self.modifier_prompt(prompt_utilisateur)
        
        # √âTAPE 2: Afficher le prompt modifi√© si debug activ√©
        if debug:
            print("\n" + "="*50)
            print("PROMPT MODIFI√â ENVOY√â √Ä GEMMA:")
            print("="*50)
            print(prompt_modifie)
            print("="*50 + "\n")
        
        # √âTAPE 3: Envoyer √† Ollama
        data = {
            "model": self.model,
            "prompt": prompt_modifie,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 500  # Limite de tokens g√©n√©r√©s
            }
        }
        
        try:
            response = requests.post(self.url_generate, json=data, timeout=60)
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Erreur {response.status_code}: {response.text}"
                
        except requests.exceptions.Timeout:
            return "Erreur: Timeout - Le mod√®le met trop de temps √† r√©pondre"
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def discuter(self, debug=False):
        """
        Boucle principale du chatbot interactif
        """
        print("="*60)
        print("ü§ñ CHATBOT GEMMA 3 1B avec modification de prompts")
        print("="*60)
        print("Commandes:")
        print("  - Tapez votre question normalement")
        print("  - 'quit' ou 'exit' pour quitter")
        print("  - 'debug' pour activer/d√©sactiver le mode debug")
        print("="*60 + "\n")
        
        while True:
            prompt_utilisateur = input("Vous: ")
            
            if prompt_utilisateur.lower() in ['quit', 'exit', 'quitter']:
                print("\nüëã Au revoir !")
                break
            
            if prompt_utilisateur.lower() == 'debug':
                debug = not debug
                print(f"\nüîß Mode debug: {'ACTIV√â' if debug else 'D√âSACTIV√â'}\n")
                continue
            
            if not prompt_utilisateur.strip():
                continue
            
            print("\nü§î Gemma r√©fl√©chit...\n")
            reponse = self.generer_reponse(prompt_utilisateur, debug=debug)
            print(f"Gemma: {reponse}\n")


# VERSION AVEC HISTORIQUE DE CONVERSATION
class ChatbotGemmaAvecHistorique(ChatbotGemma):
    """
    Version avanc√©e qui garde l'historique des conversations
    """
    
    def modifier_prompt_avec_historique(self, prompt_utilisateur: str) -> str:
        """
        Construit un prompt incluant l'historique de conversation
        """
        instructions = """Tu es un assistant IA. R√©ponds en fran√ßais.
Tiens compte de l'historique de notre conversation."""
        
        # Construction du contexte avec historique
        contexte = instructions + "\n\n"
        
        if self.historique:
            contexte += "Historique de la conversation:\n"
            for msg in self.historique[-6:]:  # Garde les 3 derniers √©changes
                role = "Utilisateur" if msg["role"] == "user" else "Assistant"
                contexte += f"{role}: {msg['content']}\n"
            contexte += "\n"
        
        contexte += f"Nouvelle question: {prompt_utilisateur}"
        
        return contexte
    
    def generer_reponse_avec_historique(self, prompt_utilisateur: str, debug=False) -> str:
        """
        G√©n√®re une r√©ponse en tenant compte de l'historique
        """
        prompt_modifie = self.modifier_prompt_avec_historique(prompt_utilisateur)
        
        if debug:
            print("\n" + "="*50)
            print("PROMPT AVEC HISTORIQUE:")
            print("="*50)
            print(prompt_modifie)
            print("="*50 + "\n")
        
        data = {
            "model": self.model,
            "prompt": prompt_modifie,
            "stream": False
        }
        
        try:
            response = requests.post(self.url_generate, json=data, timeout=60)
            
            if response.status_code == 200:
                reponse = response.json()["response"]
                
                # Mise √† jour de l'historique
                self.historique.append({"role": "user", "content": prompt_utilisateur})
                self.historique.append({"role": "assistant", "content": reponse})
                
                # Limite l'historique √† 10 messages
                if len(self.historique) > 10:
                    self.historique = self.historique[-10:]
                
                return reponse
            else:
                return f"Erreur {response.status_code}"
                
        except Exception as e:
            return f"Erreur: {str(e)}"


# EXEMPLE D'UTILISATION AVEC MODIFICATIONS PERSONNALIS√âES
class ChatbotPersonnalise(ChatbotGemma):
    """
    Exemple de chatbot avec modifications de prompt sp√©cifiques
    """
    
    def __init__(self, model_name="gemma2:2b", domaine="g√©n√©ral"):
        super().__init__(model_name)
        self.domaine = domaine
    
    def modifier_prompt(self, prompt_utilisateur: str) -> str:
        """
        Modification personnalis√©e selon le domaine
        """
        # Persona selon le domaine
        personas = {
            "code": "Tu es un expert en programmation Python.",
            "cuisine": "Tu es un chef cuisinier professionnel.",
            "sant√©": "Tu es un assistant sant√© (rappel: pas de diagnostic m√©dical).",
            "g√©n√©ral": "Tu es un assistant polyvalent."
        }
        
        persona = personas.get(self.domaine, personas["g√©n√©ral"])
        
        # Construction du prompt
        prompt_modifie = f"""{persona}

R√®gles:
- R√©ponds en fran√ßais
- Sois pr√©cis et concis
- Donne des exemples pratiques

Question: {prompt_utilisateur}"""
        
        return prompt_modifie


# UTILISATION - Choisissez votre version
if __name__ == "__main__":
    # VERSION 1: Chatbot simple avec modification de prompt
    print("D√©marrage du chatbot simple...\n")
    chatbot = ChatbotGemma(model_name="gemma3:1b")  # Changez selon votre mod√®le
    chatbot.discuter(debug=False)  # Mettez debug=True pour voir les prompts modifi√©s
    
    # VERSION 2: Chatbot avec historique (d√©commentez pour utiliser)
    # print("D√©marrage du chatbot avec historique...\n")
    # chatbot = ChatbotGemmaAvecHistorique(model_name="gemma2:2b")
    # chatbot.discuter(debug=True)
    
    # VERSION 3: Chatbot personnalis√© (d√©commentez pour utiliser)
    # print("D√©marrage du chatbot personnalis√©...\n")
    # chatbot = ChatbotPersonnalise(model_name="gemma2:2b", domaine="code")
    # chatbot.discuter()